{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pietrodileo/Python_for_MD_thesis/blob/main/TrainCNN_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D_LUG6ajBr6"
      },
      "source": [
        "# ðŸš€ Install, Import, Login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICVulkAp2WH"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Rzb8hEseZL0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9510e5-cbf6-423e-dd3a-ed61b96d70da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import wave\n",
        "import pylab\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import itertools\n",
        "import time\n",
        "import shutil\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsMikjLATdcW",
        "outputId": "c8e73608-3720-4ef0-c009-e9025a74c589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U kaleido\n",
        "#need to restart runtime after install kaleido\n",
        "import kaleido\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_DLosRPGDYt",
        "outputId": "baaccf19-9266-46f2-ede9-9937ac2a35e3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-Kj8LW0Pll-"
      },
      "source": [
        "# ðŸ“ˆ Preparing the data\n",
        "We can now load the spectrograms into memory. We use the image_dataset_from_directory utility to generate the datasets. The validation set is what will ultimately be our benchmark when becomes to performance and accuracy of our classifier. The seed is for reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create dataset functions"
      ],
      "metadata": {
        "id": "kDjam8E3v069"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "oM3R5Fh0PIbK"
      },
      "outputs": [],
      "source": [
        "def createTrain_and_Test_Dataset(directory,BATCH_SIZE,VAL_SPLIT,IMAGE_HEIGHT, IMAGE_WIDTH, MODE):\n",
        "  # Make a dataset containing the training spectrograms\n",
        "  train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                                            directory,\n",
        "                                            labels='inferred',\n",
        "                                            class_names = False,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            validation_split=VAL_SPLIT,\n",
        "                                            subset='training',\n",
        "                                            shuffle=True,\n",
        "                                            color_mode= MODE,\n",
        "                                            image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "                                            seed=0)\n",
        "\n",
        "  # Make a dataset containing the validation spectrogram\n",
        "  valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                                            directory,\n",
        "                                            labels='inferred',\n",
        "                                            class_names = False,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            validation_split=VAL_SPLIT,\n",
        "                                            subset='validation',\n",
        "                                            shuffle=True,\n",
        "                                            color_mode= MODE,\n",
        "                                            image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
        "                                            seed=0)\n",
        "  return train_dataset, valid_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHtsnmb6Prhi"
      },
      "source": [
        "Before we can build our model and start training, we need to apply one simple augmentation the dataset and that is rescaling. We convert input from int to float32 and rescale it from the (0, 255) range to the (0,1) range."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rescale data size as [256, 256]"
      ],
      "metadata": {
        "id": "13PWvHKGv7LL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "DU_FcEUaP26F"
      },
      "outputs": [],
      "source": [
        "# Function to prepare our datasets for modelling\n",
        "def prepare(batches):\n",
        "\n",
        "  def normalize(img, label):\n",
        "    return img / 255.0, label\n",
        "\n",
        "  ds = (batches\n",
        "        .map(normalize)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "        ) \n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh3M28Qsp4qf"
      },
      "source": [
        "# â° Execution time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "5wRUqDOupwVY"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-L4Yt1djL5g"
      },
      "source": [
        "# ðŸ§  Define the Model and Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model Architecture ðŸ¦¾"
      ],
      "metadata": {
        "id": "NdCQSCfYwBKc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "5F8bXADMjRZi"
      },
      "outputs": [],
      "source": [
        "def make_model(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS, N_CLASSES, num_cnn_layers):\n",
        "  # Create CNN model with 3 Convolution Layer Architecture\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
        "  # Conv2D(NumFilter, FilterSize, option...)\n",
        "  # # 1\n",
        "  # model.add(tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu')) #kernel_initializer='he_uniform'\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "  # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "  # # 2\n",
        "  # model.add(tf.keras.layers.Conv2D(64, (3, 3),strides=(1, 1), padding='same', activation='relu'))\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "  # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "  # # # 3\n",
        "  # model.add(tf.keras.layers.Conv2D(128, (3, 3),strides=(1, 1), padding='same', activation='relu'))\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "  # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "  # # # 4\n",
        "  # model.add(tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "  # model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  NUM_FILTERS = 32\n",
        "  for i in range(1, num_cnn_layers+1):\n",
        "    model.add(tf.keras.layers.Conv2D(NUM_FILTERS*i, (3,3), strides=(1, 1), activation='relu', padding='same'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  # flattening\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  # NN \n",
        "  model.add(tf.keras.layers.Dense(256, activation='relu')) \n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results function"
      ],
      "metadata": {
        "id": "qHyH-_RawF5B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "RpGQiSXAqHFw"
      },
      "outputs": [],
      "source": [
        "def plotResults(history,SaveFile,outputPath):\n",
        "  # Plot the loss curves for training and validation.\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "  epochs = range(1, len(loss_values)+1)\n",
        "  \n",
        "  d = {'Train Loss': loss_values, 'Test Loss': val_loss_values, 'Epochs':epochs}\n",
        "  df = pd.DataFrame(d)\n",
        "  \n",
        "  fig = go.Figure()\n",
        "  fig = make_subplots(rows=2, cols=1,\n",
        "                      subplot_titles=(\"Training and Validation Loss\", \"Training and Validation Accuracy\"))\n",
        "\n",
        "  fig.append_trace(go.Scatter(\n",
        "      x=df['Epochs'],\n",
        "      y=df['Train Loss'],\n",
        "      name=\"Training Loss\",       # this sets its legend entry\n",
        "      mode='lines+markers'\n",
        "  ),1,1),\n",
        "\n",
        "  fig.append_trace(go.Scatter(\n",
        "      x=df['Epochs'],\n",
        "      y=df['Test Loss'],\n",
        "      name=\"Validation Loss\",\n",
        "      mode='lines+markers'\n",
        "  ),1,1),\n",
        "\n",
        "  # Plot the accuracy curves for training and validation.\n",
        "  acc_values = history_dict['accuracy']\n",
        "  val_acc_values = history_dict['val_accuracy']\n",
        "  epochs = range(1, len(acc_values)+1)\n",
        "\n",
        "  d = {'Train Accuracy': acc_values, 'Test Accuracy': val_acc_values, 'Epochs':epochs}\n",
        "  df = pd.DataFrame(d)\n",
        "\n",
        "  fig.append_trace(go.Scatter(\n",
        "      x=df['Epochs'],\n",
        "      y=df['Train Accuracy'],\n",
        "      name=\"Training Accuracy\",       # this sets its legend entry\n",
        "      mode='lines+markers'\n",
        "  ),2,1),\n",
        "\n",
        "  fig.append_trace(go.Scatter(\n",
        "      x=df['Epochs'],\n",
        "      y=df['Test Accuracy'],\n",
        "      name=\"Validation Accuracy\",\n",
        "      mode='lines+markers'\n",
        "  ),2,1),\n",
        "\n",
        "  # edit axis labels\n",
        "  fig['layout']['xaxis']['title']='Epochs'\n",
        "  fig['layout']['xaxis2']['title']='Epochs'\n",
        "  fig['layout']['yaxis']['title']='Loss'\n",
        "  fig['layout']['yaxis2']['title']='Accuracy'\n",
        "\n",
        "  # Tick Distance\n",
        "  fig['layout']['xaxis']['dtick']= 5\n",
        "  fig['layout']['xaxis2']['dtick']= 5\n",
        "  # First value on x axis\n",
        "  fig['layout']['xaxis']['tick0']= 0\n",
        "  fig['layout']['xaxis2']['tick0']= 0\n",
        "  # Tick Mode\n",
        "  fig['layout']['xaxis']['tickmode']= 'linear'\n",
        "  fig['layout']['xaxis2']['tickmode']= 'linear'\n",
        "\n",
        "  fig.update_layout(\n",
        "      height=800, \n",
        "      width=1200,\n",
        "      title=\"Loss and Accuracy\",\n",
        "      legend_title=\"Dataset\",\n",
        "      font=dict(size=14)\n",
        "  )\n",
        "\n",
        "  return fig, loss_values, val_loss_values, acc_values, val_acc_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt_EXMHB1aoy"
      },
      "source": [
        "# 3. ðŸ›« Compute Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v7ePYXsSfCp"
      },
      "source": [
        "## Define Parameters for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "NEzN4dZhSeFV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Declare Constants\n",
        "#@markdown ---\n",
        "TASK = '100EP_2conv_Batch32_adam' #@param {type: \"string\"}\n",
        "num_cnn_layers = 2 #@param {type: \"number\"}\n",
        "IMAGE_HEIGHT = 256 #@param {type: \"number\"}\n",
        "IMAGE_WIDTH = 256 #@param {type: \"number\"}\n",
        "BATCH_SIZE = 32 #@param {type: \"slider\", min: 1, max: 256}\n",
        "EPOCHS = 100  #@param {type: \"number\"}\n",
        "VAL_SPLIT = 0.2  #@param {type: \"slider\", min: 0, max: 1, step:0.05}\n",
        "MODE = \"grayscale\"  #@param ['rgb', 'rgba', 'grayscale']\n",
        "SaveFile = True #@param {type: \"boolean\"}\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z64G6e4vSboI"
      },
      "source": [
        "##Select Input and Output â\n",
        "\n",
        "Speed up Google Colab by copying the zip file containing the images directly on the local path (75% faster). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "4cYlgi69qoJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fba707-98d4-4ca3-af92-cdbe95538b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already transferred from Drive\n",
            "Files already unzipped\n"
          ]
        }
      ],
      "source": [
        "fileName = 'C1'\n",
        "zipfile = fileName + '.zip'\n",
        "directory = '/content/drive/MyDrive/TesiMagistrale/outputSpectrogram/CNN_TrainingIMGs/BW/'\n",
        "\n",
        "zipPath = os.path.join(directory,zipfile)\n",
        "outputDir = os.path.join(directory,'Risultati')\n",
        "outputPath = os.path.join(outputDir,TASK)\n",
        "\n",
        "if not os.path.exists(outputPath):\n",
        "   # Create a new directory because it does not exist\n",
        "   os.makedirs(outputPath)\n",
        "   print(\"The output folder has been created!\")\n",
        "\n",
        "# Location of Zip File\n",
        "drive_path = zipPath\n",
        "local_path = '/content'\n",
        "\n",
        "zipCopyPath = os.path.join(local_path,zipfile)\n",
        "if not os.path.exists(zipCopyPath):\n",
        "  # Copy the zip file and move it up one level (AKA out of the drive folder)\n",
        "  !cp '{drive_path}' .\n",
        "else:\n",
        "  print('Files already transferred from Drive')\n",
        "\n",
        "if not os.path.exists(fileName):\n",
        "  # Navigate to the copied file and unzip it quietly\n",
        "  os.chdir(local_path)\n",
        "  !unzip -q '{zipfile}'\n",
        "else:\n",
        "  print('Files already unzipped')\n",
        "# change directory to the new one\n",
        "NewDir = os.path.join(local_path,fileName)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Select channels and classes, split the dataset"
      ],
      "metadata": {
        "id": "gatXFcruvUNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select number of channels\n",
        "if MODE == 'grayscale':\n",
        "  N_CHANNELS = 1\n",
        "elif MODE == 'rgb':\n",
        "  N_CHANNELS = 3\n",
        "elif MODE == 'rgba':\n",
        "  N_CHANNELS = 4\n",
        "\n",
        "# Create the dataset and select number of classes\n",
        "[train_batches, valid_batches] = createTrain_and_Test_Dataset(NewDir,BATCH_SIZE,VAL_SPLIT,IMAGE_HEIGHT, IMAGE_WIDTH, MODE)\n",
        "\n",
        "classNames = train_batches.class_names\n",
        "N_CLASSES = len(classNames)\n",
        "\n",
        "# Extract image path for training and validation set\n",
        "image_paths_train = train_batches.file_paths\n",
        "image_paths_valid = valid_batches.file_paths\n",
        "\n",
        "train_dataset = prepare(train_batches)\n",
        "valid_dataset = prepare(valid_batches)\n",
        "\n",
        "LabelNames = valid_batches.class_names\n",
        "LabelNames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwckqaCNuyF9",
        "outputId": "a5c47822-ed8b-415f-efea-65ab9f66f257"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1192 files belonging to 2 classes.\n",
            "Using 954 files for training.\n",
            "Found 1192 files belonging to 2 classes.\n",
            "Using 238 files for validation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['class_HealthyControl', 'class_advanced_PD-OFF']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Optimizer and Loss Function â™Ÿ\n",
        "### LOSS FUNCTION\n",
        "see: \n",
        "* tf.keras.losses\n",
        "\n",
        "### OPTIMIZERS \n",
        "See: \n",
        "* tf.keras.optimizers.RMSprop()\n",
        "* tf.keras.optimizers.SGD\n",
        "* tf.keras.optimizers.Adam\n",
        "\n",
        "We introduce **Early Stopping**, which has the following parameters:\n",
        "* min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
        "* patience: Number of epochs with no improvement after which training will be stopped\n",
        "* mode: One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing; in \"max\" mode it will stop when the quantity monitored has stopped increasing; in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n",
        "* baseline: Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline.\n",
        "* restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used. An epoch will be restored regardless of the performance relative to the baseline. If no epoch improves on baseline, training will run for patience epochs and restore weights from the best epoch in that set.\n",
        "\n",
        "We also introduce **ModelCheckpoint**, which save in a specified path the model that achieved the best results and **ReduceLROnPlateau**, which reduce learning rate when a metric has stopped improving.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dMBqw03u7XJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "sjKWPceKkTFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65e44be-e04c-413a-9f77-b5f3bd167175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "monitorValue = 'val_loss'\n",
        "monitorMode = 'min'\n",
        "\n",
        "checkpoint_path = os.path.join(outputPath,'training_1/cp.ckpt')\n",
        "bestModelName = os.path.join(outputPath,'best_mdl.h5')\n",
        "earlyStopping = EarlyStopping(monitor=monitorValue, \n",
        "                              patience=40, \n",
        "                              verbose=0, \n",
        "                              mode=monitorMode,\n",
        "                              restore_best_weights=True)\n",
        "\n",
        "mcp_save = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                           save_best_only=True, \n",
        "                           monitor=monitorValue, \n",
        "                           mode=monitorMode)\n",
        "\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor=monitorValue,\n",
        "                                   factor=0.1, patience=5, \n",
        "                                   verbose=0, min_delta=1e-4, mode=monitorMode)\n",
        "\n",
        "LOSS = 'sparse_categorical_crossentropy'\n",
        "\n",
        "# OPTIMIZER = tf.keras.optimizers.SGD(\n",
        "#         learning_rate=0.01,\n",
        "#         momentum=0.0,\n",
        "#         nesterov=False,\n",
        "#         name='SGD')\n",
        "\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELEmc60XS1m-"
      },
      "source": [
        "## Train the network ðŸ’ª\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Model"
      ],
      "metadata": {
        "id": "kgnVt1KpG0LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_model(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS, N_CLASSES, num_cnn_layers)\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss = LOSS,\n",
        "    optimizer = OPTIMIZER,\n",
        "    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "0PJNDHxTGcUb"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "Ctmf-fIEG3gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model and capture the history\n",
        "history = model.fit(train_dataset, \n",
        "                    epochs=EPOCHS, \n",
        "                    validation_data = valid_dataset,\n",
        "                    callbacks = [earlyStopping, mcp_save, reduce_lr_loss]\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9ySSk6M7QWa",
        "outputId": "842adf77-038d-40ed-de3a-1eceea022e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.0174 - accuracy: 0.6499"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r30/30 [==============================] - 31s 847ms/step - loss: 1.0174 - accuracy: 0.6499 - val_loss: 0.9977 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.8145"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r30/30 [==============================] - 31s 882ms/step - loss: 0.4211 - accuracy: 0.8145 - val_loss: 0.8917 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 25s 609ms/step - loss: 0.2792 - accuracy: 0.9036 - val_loss: 1.3021 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 23s 610ms/step - loss: 0.1628 - accuracy: 0.9403 - val_loss: 1.0676 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 23s 613ms/step - loss: 0.1042 - accuracy: 0.9612 - val_loss: 1.0261 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9748"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r30/30 [==============================] - 29s 830ms/step - loss: 0.0817 - accuracy: 0.9748 - val_loss: 0.7575 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 26s 733ms/step - loss: 0.0448 - accuracy: 0.9853 - val_loss: 0.9849 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 23s 634ms/step - loss: 0.0202 - accuracy: 0.9969 - val_loss: 1.1266 - val_accuracy: 0.5798 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9937"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r30/30 [==============================] - 29s 837ms/step - loss: 0.0248 - accuracy: 0.9937 - val_loss: 0.6283 - val_accuracy: 0.5924 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9937"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r30/30 [==============================] - 30s 876ms/step - loss: 0.0235 - accuracy: 0.9937 - val_loss: 0.6127 - val_accuracy: 0.6008 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 26s 682ms/step - loss: 0.0168 - accuracy: 0.9979 - val_loss: 1.2711 - val_accuracy: 0.5798 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results ðŸ›¬ "
      ],
      "metadata": {
        "id": "3pz74C_nwTWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcZCdu8qoXdd"
      },
      "outputs": [],
      "source": [
        "fig, loss_values, val_loss_values, acc_values, val_acc_values = plotResults(history,SaveFile,outputPath)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save plot"
      ],
      "metadata": {
        "id": "aLq8gdyywV62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnMBGtUzDVHr"
      },
      "outputs": [],
      "source": [
        "# Save pic in HTML\n",
        "if SaveFile == True:\n",
        "  AccLossPngOutput = os.path.join(outputPath,'results.html')\n",
        "  fig.write_html(AccLossPngOutput)\n",
        "  print('Image Saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOm5LjC4c346"
      },
      "outputs": [],
      "source": [
        "# Convert HTML in PNG and export to Google Drive\n",
        "resultImg = f\"{outputPath}\"+'/results.png'\n",
        "fig.to_image(format=\"png\", engine=\"kaleido\");\n",
        "fig.write_image(resultImg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psSaogYhS94c"
      },
      "source": [
        "##Export Results to Google Drive ðŸ’¯"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the best model"
      ],
      "metadata": {
        "id": "aTI0C_TAVKnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(bestModelName)"
      ],
      "metadata": {
        "id": "PA7w_vXeVRng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import a previous model"
      ],
      "metadata": {
        "id": "rOE9afI7IAUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(bestModelName)"
      ],
      "metadata": {
        "id": "WYZBkc52H_lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the final loss and accuracy"
      ],
      "metadata": {
        "id": "td6Iu1zHXkQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wZf15D4QHVy"
      },
      "outputs": [],
      "source": [
        "final_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\n",
        "print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute runtime"
      ],
      "metadata": {
        "id": "ij9d0ny9XoUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGIMxaGmYvsM"
      },
      "outputs": [],
      "source": [
        "runtime = time.time() - start_time\n",
        "\n",
        "print('Runtime:')\n",
        "print(\"--- %s seconds ---\" % (runtime))\n",
        "print(\"--- %s minutes ---\" % ((runtime)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68lcys2S5p9E"
      },
      "outputs": [],
      "source": [
        "# dictionary of lists  \n",
        "InfoDict = {'IMAGE_HEIGHT':[IMAGE_HEIGHT], 'IMAGE_WIDTH':[IMAGE_WIDTH],\n",
        "              'BATCH_SIZE': [BATCH_SIZE], 'N_CHANNELS': [N_CHANNELS], 'N_CLASSES': [N_CLASSES],\n",
        "              'EPOCHS':[EPOCHS], 'VAL_SPLIT': [VAL_SPLIT], 'MODE': MODE,\n",
        "              'FINAL VALIDATION LOSS': [final_loss], 'FINAL VALIDATION ACC': [final_acc],\n",
        "              'RUNTIME (s)': [runtime], 'RUNTIME (min)': [runtime/60]\n",
        "              }\n",
        "InfoDict\n",
        "dfInfo = pd.DataFrame(InfoDict)\n",
        "dfInfo.index = ['CNN']\n",
        "# saving the dataframe \n",
        "outputName = 'Model_Info_and_Performance.xlsx'\n",
        "OutputFileName = os.path.join(outputPath,outputName)\n",
        "if SaveFile == True:\n",
        "  # writing to Excel\n",
        "  dfInfo.to_excel(OutputFileName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4K3KSjgzu7C"
      },
      "outputs": [],
      "source": [
        "# dictionary of lists  \n",
        "OutputDict = {'Train_ACC':acc_values, 'Train_LOSS':loss_values, \n",
        "              'Test_ACC': val_acc_values, 'Test_LOSS': val_loss_values}\n",
        "dfResult = pd.DataFrame(OutputDict) \n",
        "# saving the dataframe \n",
        "outputName = 'Risultati'\n",
        "OutputFileName = outputPath+'/'+outputName+'.xlsx'\n",
        "dfResult.to_excel(OutputFileName) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJnUc6p7ll4Z"
      },
      "outputs": [],
      "source": [
        "Summary = os.path.join(outputPath,'modelsummary.txt')\n",
        "modelInfo = model.summary()\n",
        "\n",
        "stringlist = []\n",
        "model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "short_model_summary = \"\\n\".join(stringlist)\n",
        "print(short_model_summary)\n",
        "\n",
        "with open(Summary, 'w') as f:\n",
        "  # Pass the file handle in as a lambda function to make it callable\n",
        "  f.write(short_model_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYy0getAmjMT"
      },
      "outputs": [],
      "source": [
        "pngOutput = os.path.join(outputPath,'model_plot.png')\n",
        "plot_model(model, to_file=pngOutput, show_shapes=True, show_layer_names=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "trADRP8vP0VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluate_Batches(batch_dataset):\n",
        "  #extract the labels of each batch\n",
        "  true_labels = []\n",
        "  pred_labels = [] \n",
        "  train_labels = []\n",
        "  for image_batch, label_batch in batch_dataset:\n",
        "    # image_batch contains n figure, where n is batch size.  \n",
        "    # It's an EagerTensor with a shape of (n, ImHeight, ImWidth, Nchannels)\n",
        "    preds = model.predict(image_batch,verbose=0)\n",
        "    # preds is a ndarray with n rows and N_CLASSES columns\n",
        "    # it is the output of the last neurons of the Dens/softmax Layer\n",
        "    pred_labels += np.argmax(preds, axis = - 1).tolist()\n",
        "    train_labels += label_batch.numpy().tolist()\n",
        "    # Alternative Code (use EagerTensor, a bit longer) \n",
        "    # pred_labels.append(np.argmax(preds, axis = - 1))\n",
        "    # pred_labels is a list of which each element is an ndarray with size n (batchsize)\n",
        "    # This list is composed by M elements, where M is the number of batch \n",
        "  return train_labels, pred_labels\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=18)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45, fontsize=13)\n",
        "        plt.yticks(tick_marks, target_names, fontsize=13)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     size='xx-large',\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]), \n",
        "                     size='xx-large',\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label',fontsize=14)\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass),fontsize=14)\n",
        "    plt.show()    \n",
        "\n",
        "def ConfMtxPlot(train_labels, pred_labels):\n",
        "  confusion_mtx = confusion_matrix(train_labels, pred_labels) \n",
        "  # plot the confusion matrix\n",
        "  f,ax = plt.subplots(figsize=(8, 7))\n",
        "  sns.heatmap(confusion_mtx, annot=True, linewidths=0.1,cmap=\"Blues\",linecolor=\"k\", fmt= '.0f',ax=ax,cbar=False)\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JmQQTIKDTf0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels, true_labels = Evaluate_Batches(valid_dataset)"
      ],
      "metadata": {
        "id": "417r-KpCpnrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_mtx = confusion_matrix(true_labels, pred_labels)\n",
        "plot_confusion_matrix(confusion_mtx,\n",
        "                          LabelNames,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=False)"
      ],
      "metadata": {
        "id": "Q6FQb_V79giZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the loss and accuracy curves for training and validation \n"
      ],
      "metadata": {
        "id": "7XR6YHBD90Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], color='g', label=\"Training Loss\")\n",
        "plt.plot(history.history['val_loss'], color='b', label=\"Validation Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mOYv_67X8kCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_labels, pred_labels, target_names = LabelNames))"
      ],
      "metadata": {
        "id": "MVWd4j0fkqOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC Curve"
      ],
      "metadata": {
        "id": "yi9jZcxgtQ_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "FPR, TPR, ROC_Thresholds = roc_curve(true_labels, pred_labels)\n",
        "AUC = auc(FPR, TPR)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(FPR, TPR, label='AUC = {:.3f}'.format(AUC))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WlEIjX9ZtQVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics for Binary classifications"
      ],
      "metadata": {
        "id": "ZiZFeblVqjmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PositiveClass = 1 # select positive class (disease), 0 or 1, in binary classification\n",
        "\n",
        "if len(LabelNames) == 2:\n",
        "  if PositiveClass == 0:\n",
        "    TP = confusion_mtx[0,0]\n",
        "    FP = confusion_mtx[0,1]\n",
        "    FN = confusion_mtx[1,0]\n",
        "    TN = confusion_mtx[1,1]\n",
        "  else:\n",
        "    TN = confusion_mtx[0,0]\n",
        "    FN = confusion_mtx[0,1]\n",
        "    FP = confusion_mtx[1,0]\n",
        "    TP = confusion_mtx[1,1]\n",
        "  \n",
        "  Accuracy = (TP+TN)/(TP+TN+FP+FN);\n",
        "  Miscl_Rate = 1-Accuracy;\n",
        "  PPV = TP/(TP+FP); # also called Precision\n",
        "  NPV = TN/(TN+FN);\n",
        "  Sensitivity = TP/(TP+FN); # Or Recall, True Positive Rate\n",
        "  Specificity = TN/(TN+FP); # True Negative Rate\n",
        "  J = Sensitivity+Specificity-1;\n",
        "  F1score = 2*((PPV*Sensitivity)/(PPV + Sensitivity));\n",
        "  Gmean = np.sqrt(Sensitivity*Specificity);\n",
        "\n",
        "  true_labels = np.array(true_labels)\n",
        "  pred_labels = np.array(pred_labels)\n",
        "  errors = true_labels != pred_labels\n",
        "  errorsNum = sum(errors)\n",
        "\n",
        "  StatLabels = [\"Accuracy\",\"Misclassification Rate\",\"PPV\",\"NPV\",\"Sensitivity\",\n",
        "                \"Specificity\",\"AUC\",\"Youden's Index\",\n",
        "                \"F1-score\",\"G-Mean\",\"Runtime (s)\"]\n",
        "\n",
        "  StatArray = np.array([Accuracy,Miscl_Rate,PPV,NPV,Sensitivity,Specificity,\n",
        "               AUC,J,F1score,Gmean, runtime])\n",
        "\n",
        "  Stat_df = pd.DataFrame(StatArray)#,columns=StatLabels)\n",
        "  Stat_df = pd.DataFrame.transpose(Stat_df)\n",
        "  Stat_df.columns = StatLabels\n",
        "  Stat_df.index = ['Statistics']\n",
        "Stat_df\n",
        "# # Standard Error\n",
        "# q1 = AUC/(2-AUC);\n",
        "# q2 = 2*(AUC)^2/(1+AUC);\n",
        "# SE = sqrt((AUC*(1-AUC)+(na-1)*(q1-AUC^2)+(nn-1)*(q2-AUC^2))/(na*nn));"
      ],
      "metadata": {
        "id": "U5boyLDNk8YR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qHyH-_RawF5B"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}